Wesley Minner & Christie Mathews 
  703549234       104404412

CS 111 Operating Systems Principles
Professor Eggert
Winter 2016

Spec: http://web.cs.ucla.edu/classes/winter16/cs111/assign/lab4.html

================================================================================
Lab 4 - Synchronization
================================================================================

Makefile functionality:
* make: compiles programs 'addtest' and 'sltest' for part 1 and 2 of the lab
* make dist: packages files into tarball and does some checks
* make check: executes test.sh, which performs some simple tests on addtest and 
	sltest
* make clean: removes all files created from 'make'
* make realclean: removes everything 'make clean' removes, then also removes
	.pdf files and .dat files generated from 'make log' and 'make graph'
* make log: runs logdata.sh, which collects data for graphs and puts them in 
	.dat files (of the form p#d#.dat).  Each value of time per operation in the
	.dat files is an average of 5 runs.
* make graph: runs the gnuplot script graphdata.gp, which generates graphs from
	the data collected by logdata.sh
* make addtest: only compiles program 'addtest' for part 1 of the lab
* make sltest: only compiles program 'sltest' for part 2 of the lab


addtest functionality (main_add.c):
* Working options
	--threads=# (default 1)
	--iterations=# (default 1)
	--iter=# (same as above)
	--yield=# (0 or 1, default 0)
	--sync=m (protect with mutex)
	--sync=s (protect with spin-lock)
	--sync=c (protect with compare-and-swap)
* Exits with non-zero status if final count not equal to zero


sltest functionality (main_sl.c):
* Working options
	--threads=#
	--iterations=#
	--iter=# (same as above)
	--yield=[ids]
	--sync=m (protect with mutex)
	--sync=s (protect with spin-lock)
	--lists=#
* Exits with non-zero status if final list length not equal to zero


Sorted doubly linked list functionality (main_sl.c):
	insert
	delete
	lookup
	length


Graph Deliverables
* Part 1
	p1d1.pdf: average time per operation vs the number of iterations
	p1d2-5.pdf: average time per operation vs the number of threads for all four
		versions of the add function. See separate graphs of each data set in 
		p1d2.pdf, p1d3.pdf, p1d4.pdf, and p1d5.pdf.
* Part 2
	p2d6.pdf: average time per unprotected operation vs number of iteration
		(single thread)
	p2d7-9.pdf: average time per operation (for none, mutex, and spin-lock) vs 
		number of threads. See separate graphs of each data set in p2d7.pdf,
		p2d8.pdf, and p2d9.pdf.
	p2d10-12.pdf: average time per operation (for none, mutex, and spin-lock)
		vs the ratio of threads per list. See separate graphs of each data set
		in p2d10.pdf, p2d11.pdf, and p2d12.pdf.


Part 1 TODO:
* Short answers to questions posed in spec


Part 2 TODO:
* Short answers to questions posed in spec


Part 3 TODO:
* Short answers to questions posed in spec


Notes:
* N/A


Known Issues:
* N/A

================================================================================
Answers
================================================================================
Questions 1.1
=============

1. Why does it take this many threads or iterations?

From experimental testing on Seasnet, using greater than 400 iterations and 10
threads results in a failure "fairly" consistently. Using a single thread will
never failure (as there can be no race conditions). Decreasing the number of
threads improves the chances of getting a final count of 0 with 400 iterations,
but not significantly. Increasing the number of threads offers the same rate of
failure as 400 x 10 (consistent failure).

It most likely takes this many iterations and threads because the more chances
we have to witness a race condition, the more likely it will happen. The
scheduler must preempt a running thread while it is in the middle of the add
function and this rarely happens if there are few iterations or threads trying
to perform operations at the same time.

2. Why does a significantly smaller number of iterations so seldom fail?

With fewer iterations, threads don't end up waiting a long time to take their
turn. Thus the scheduler rarely has to preempt active threads to serve threads
that have been waiting a long time.

Questions 1.2
=============

1. Why does the average cost per operation drop with increasing iterations?

There is overhead associated with creating the single thread that is constant
for this data set. The overhead is measured by the timer and makes up the
majority of the measured time for a small number of iterations. By increasing
iterations, the overhead of creating the thread becomes smaller relative to the
time spent doing the add operations, such that there are a lot more operations,
but the additional add operations were all very "cheap".

Creating the thread was an "expensive" operation relative to the iterations of
the add function, so the average cost per operation goes down as more cheap
operations are added.

2. How do we know what the “correct” cost is?

The correct cost would be how much time it takes to perform just useful work,
without the overhead costs. In this case, that would be just measuring the add
operation time of each thread. We can find this value by measuring the cost per
operation as iterations goes to infinity.  The constant overhead cost of setting
up the threads then becomes negligible, since the add cost time is so much
larger.

3. Why are the --yield runs so much slower? Where is the extra time going?

The tests with yield run slower due to thread context switching. This is true
regardless of how many threads you have. The worker child thread has to yield
in the middle of its add operation every iteration, which adds a context switch
overhead cost to each add operation. The thread context switch is faster than
a process context switch, but it still wastes time not doing useful "work".

Apart from the thread context switch, there also may be less efficient cache
usage as another thread being switched to might have poor temporal or spatial
locality, causing cache misses more often.

4. Can we get valid timings if we are using --yield? How, or why not?

There will always be overhead due to the thread context switch. Taking the limit
as iterations goes to infinity will NOT give us valid timings because the number
of thread context switches also increases to infinity as well. Compare this to
the thread creation overhead, which remains constant and independent of the
number of iterations. Therefore we cannot get valid timings with the --yield
option.

Questions 1.3
=============

1. Why do all of the options perform similarly for low numbers of threads?

	For the single thread case, the additional synchronization code provides
	very little performance degradation because there is no thread competition.
	The single thread gains the lock or gets through the sync while loop on its
	first try every time. As a result, the performance is nearly identical to
	the tests with synchronization off.

	As the number of threads increase, there is increased competition to gain
	the lock, and some threads will waste time waiting for other threads to exit
	critical sections of the code. That additional wasted overhead makes
	synchronization tests take significantly longer than the tests without
	synchronization for high numbers of threads.

2. Why do the three protected operations slow down as the number of threads 
   rises?

	The protected operations have threads waiting for other threads to leave
	critical sections. Depending on the synchronization implementation, this can
	be efficent or very inefficient (like the spin-locks). Compare and swap does
	not force threads to wait for critical sections to be clear, but instead
	allows them to proceed and only apply the final operation on the shared
	count variable if a race condition  has not occurred. This is more efficient
	than the other synchronization types.

	Either way, more threads creates more chances for waiting at critical
	sections, and thus more context switches and preemption.

3. Why are spin-locks so expensive for large numbers of threads?

	Spin-locks do not set the threads as blocked, but instead have the threads
	continue to "spin" in a while loop. These are wasted operations that
	accomplish nothing, and thus waste large amounts of time relative to the
	other synchronization methods.

	Mutexes block the waiting threads, and wake them back up when the mutex is
	released, which is more efficient and doesn't waste as many operations.
	Compare and swap doesn't have threads wait at all, so it could waste
	operations if a race condition happens. However this is not a guarantee and
	some of the critical sections with multiple threads in them do succeed,
	unlike the spin-lock case where time is always wasted if another thread is
	in the critical section.

Questions 2.1
=============

1. Explain the variation in time per operation vs the number of iterations? How 
   would you propose to correct for this effect?

    Time per operation is very high for a small number of iterations, but drops
    off rapidly as iterations increase.  This is similar to part 1 in that the
    overhead of creating threads is high compared to total runtime of the
    program. Increasing the amount of iterations increases the number of "cheap"
    operations, thus reducing the average time per operation.

    The graph exhibits some jumps in average time per operation. This is likely
    due to the random keys assigned to each list element being inserted/deleted.
    Since the list is sorted, a thread may have a much longer insert or lookup
    time for some keys (if the key value is high).  This element of randomness
    may be reduced by running each test multiple times.  For our graph, for each
    number of iterations tested, data was collected for five trials and average
    was taken.  This helped smooth out the graph, but a few bumps still remain.

    By running more trials per each number of iterations, the true value can be
    found (N/2 list traversal for a list of N elements).

Questions 2.2
=============

1. Compare the variation in time per protected operation vs the number of 
   threads in Part 2 and in Part 1. Explain the difference.

    

Questions 2.3
=============

1. Explain the the change in performance of the synchronized methods as a 
   function of the number of threads per list.


2. Explain why threads per list is a more interesting number than threads (for 
   this particular measurement).



Questions 3
===========

1. Why must the mutex be held when pthread_cond_wait is called?


2. Why must the mutex be released when the waiting thread is blocked?


3. Why must the mutex be reacquired when the calling thread resumes?


4. Why must this be done inside of pthread_cond_wait? Why can’t the caller 
   simply release the mutex before calling pthread_cond_wait?


5. Can this be done in a user-mode implementation of pthread_cond_wait? If so, 
   how? If it can only be implemented by a system call, explain why?

